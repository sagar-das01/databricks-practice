# databricks-practice## Databricks CLI Commands Reference> **⚠️ Important:** Always use quotes around workspace paths, especially if they contain spaces or special characters like `@`.### Notebook File ExtensionsDatabricks notebooks use language-specific file extensions:| Language | File Extension | Example ||----------|----------------|---------|| Python   | `.py`         | `notebook.py` || SQL      | `.sql`        | `query.sql` || Scala    | `.scala`      | `notebook.scala` || R        | `.r`          | `notebook.r` |**Special Formats:**- **`.dbc`** - Databricks Archive (can contain multiple notebooks and directories)- **`.ipynb`** - Jupyter notebook format (when using `--format JUPYTER`)When exported with `--format SOURCE`, notebooks include special comments like `# Databricks notebook source` to preserve metadata.### Creating Cells in Notebook FilesDatabricks notebooks use special comment markers to separate cells:#### Cell Separator```python# COMMAND ----------```**Example Python Notebook:**```python# Databricks notebook sourceprint("This is cell 1")# COMMAND ----------print("This is cell 2")# COMMAND ----------x = 10print(f"This is cell 3: {x}")```#### Magic CommandsYou can use magic commands to change the cell type:**Markdown Cell:**```python# MAGIC %md# MAGIC # Heading# MAGIC This is **markdown** text```**SQL Cell (in Python notebook):**```python# MAGIC %sql# MAGIC SELECT * FROM table_name```**Other Magic Commands:**- `%md` - Markdown- `%sql` - SQL- `%scala` - Scala (in Python notebooks)- `%python` - Python (in Scala/R notebooks)- `%r` - R- `%sh` - Shell commands- `%fs` - File system commands- `%run` - Run another notebook**Example with Multiple Cell Types:**```python# Databricks notebook source# Python code celldata = [1, 2, 3, 4, 5]# COMMAND ----------# MAGIC %md# MAGIC ## Data Analysis# MAGIC Let's analyze our data# COMMAND ----------# MAGIC %sql# MAGIC SELECT COUNT(*) FROM my_table# COMMAND ----------# Back to Pythonprint("Analysis complete!")```### Creating Folders in WorkspaceCreate a folder in the Databricks workspace:```bashdatabricks workspace mkdirs '<path>'```Example:```bashdatabricks workspace mkdirs '/Users/sagarrdass@outlook.com/databricks-practice/Databricks Data Engineer Professional practice'```### Importing Files/Notebooks to Workspace**Upload from local → Databricks workspace**#### Import a single notebook (Python, SQL, Scala, or R):```bashdatabricks workspace import <source-file> '<workspace-path>' --language <LANGUAGE> --format SOURCE```Examples:```bash# Import a Python notebookdatabricks workspace import notebook.py '/Users/sagarrdass@outlook.com/databricks-practice/notebook' --language PYTHON --format SOURCE# Import a SQL notebookdatabricks workspace import query.sql '/Users/sagarrdass@outlook.com/databricks-practice/query' --language SQL --format SOURCE# Import a Scala notebookdatabricks workspace import notebook.scala '/Users/sagarrdass@outlook.com/databricks-practice/notebook' --language SCALA --format SOURCE```#### Import a directory of notebooks (as ZIP):```bashdatabricks workspace import <source.zip> '<workspace-path>' --format SOURCE```Example:```bashdatabricks workspace import notebooks.zip '/Users/sagarrdass@outlook.com/databricks-practice' --format SOURCE```#### Import as DBC (Databricks Archive):```bashdatabricks workspace import <source.dbc> '<workspace-path>' --format DBC```### Exporting Files/Notebooks from Workspace**Download from Databricks workspace → local**#### Export a single notebook:```bashdatabricks workspace export '<workspace-path>' --file <local-file> --format SOURCE```Examples:```bash# Export a notebook to local filedatabricks workspace export '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --file practice-notebook-1.py --format SOURCE# Export as DBC archivedatabricks workspace export '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --file notebook.dbc --format DBC# Export entire directory as DBCdatabricks workspace export-dir '/Users/sagarrdass@outlook.com/databricks-practice' ./local-folder --format DBC```#### Export entire directory:```bashdatabricks workspace export-dir '<workspace-path>' <local-directory> --format SOURCE```Example:```bash# Export all notebooks from workspace folder to local folderdatabricks workspace export-dir '/Users/sagarrdass@outlook.com/databricks-practice' ./my-notebooks --format SOURCE```### Other Useful Workspace Commands**List workspace contents:**```bashdatabricks workspace ls '<path>'databricks workspace ls -l '<path>'  # Long format with details```**Delete workspace object:**```bashdatabricks workspace delete '<path>'databricks workspace delete '<path>' --recursive  # Delete directory recursively```**Get status of workspace object:**```bashdatabricks workspace get-status '<path>'```### Troubleshooting**Error: "expected to have the absolute path of the object or directory"**This error occurs when:1. The workspace path is not properly quoted (paths containing `@` symbols or special characters must be quoted)2. You're using the old CLI syntax instead of the `--file` flag**Solution:**```bash# ❌ Wrong - old syntax without --file flagdatabricks workspace export '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' practice-notebook-1.py --format SOURCE# ✅ Correct - new syntax with --file flagdatabricks workspace export '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --file practice-notebook-1.py --format SOURCE```**Error: "The zip file may not be valid..."**This error occurs when:1. You're trying to import without specifying a source file2. You're importing a single file without specifying the `--language` flag3. The file format doesn't match what Databricks expects**Solution:**- First create the folder: `databricks workspace mkdirs '<path>'`- Then import files with proper language specification: `databricks workspace import <file> '<path>' --language PYTHON --format SOURCE`---## Syncing Databricks Notebooks with GitHubThere are several methods to sync your Databricks notebooks with GitHub:### Method 1: Databricks Repos (Recommended)Databricks Repos provides native Git integration directly in the Databricks workspace.**Setup Databricks Repos:**1. **In Databricks UI:**   - Go to **Repos** in the left sidebar   - Click **Add Repo**   - Enter your GitHub repository URL   - Authenticate with GitHub (using Personal Access Token)   - Clone the repository2. **Work with notebooks in the Repo:**   - Create/edit notebooks directly in the Repos folder   - Changes are tracked by Git   - Use the UI to commit, push, pull, and create branches**CLI Commands for Repos:**```bash# List reposdatabricks repos list# Create a new repodatabricks repos create --url https://github.com/username/repo-name --provider gitHub --path /Repos/username/repo-name# Update repo (pull changes)databricks repos update <repo-id> --branch main# Get repo infodatabricks repos get <repo-id>```### Method 2: Export Notebooks + Git (Manual Sync)Export notebooks to your local machine and use standard Git workflow.**Step 1: Export notebooks from Databricks**```bash# Export single notebookdatabricks workspace export '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --file practice-notebook-1.py --format SOURCE# Export entire directorydatabricks workspace export-dir '/Users/sagarrdass@outlook.com/databricks-practice' ./notebooks --format SOURCE```**Step 2: Commit and push to GitHub**```bashgit add .git commit -m "Update notebooks from Databricks"git push origin main```**Step 3: Import changes back to Databricks (after pulling from GitHub)**```bash# Pull latest changesgit pull origin main# Import updated notebook to Databricksdatabricks workspace import practice-notebook-1.py '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --language PYTHON --format SOURCE```### Method 3: Automated Sync with CI/CDUse GitHub Actions to automatically sync notebooks.**Example GitHub Action Workflow (`.github/workflows/sync-databricks.yml`):**```yamlname: Sync to Databrickson:  push:    branches: [main]jobs:  deploy:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v3            - name: Install Databricks CLI        run: |          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh            - name: Configure Databricks CLI        env:          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}        run: |          echo "[DEFAULT]" > ~/.databrickscfg          echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg          echo "token = $DATABRICKS_TOKEN" >> ~/.databrickscfg            - name: Deploy notebooks        run: |          databricks workspace import practice-notebook-1.py '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --language PYTHON --format SOURCE --overwrite```**Required GitHub Secrets:**- `DATABRICKS_HOST`: Your Databricks workspace URL (e.g., `https://adb-1234567890123456.7.azuredatabricks.net`)- `DATABRICKS_TOKEN`: Your Databricks personal access token### Method 4: Using dbx (Databricks CLI Extension)`dbx` is a Databricks CLI extension for advanced deployment workflows.```bash# Install dbxpip install dbx# Initialize dbx projectdbx configure# Deploy to Databricksdbx deploy```### Best Practices for Git + Databricks1. **Use `.gitignore`:**   ```   # Databricks   .databricks/      # Python   __pycache__/   *.pyc   .pytest_cache/      # IDE   .idea/   .vscode/   ```2. **Organize notebooks by project/feature:**   ```   notebooks/   ├── data-ingestion/   │   ├── load-raw-data.py   │   └── validate-data.py   ├── transformations/   │   └── clean-data.py   └── analysis/       └── generate-report.py   ```3. **Use meaningful commit messages:**   ```bash   git commit -m "Add data validation notebook"   git commit -m "Fix SQL query in analysis notebook"   ```4. **Create branches for development:**   ```bash   git checkout -b feature/new-analysis   # Make changes   git commit -m "Add new analysis notebook"   git push origin feature/new-analysis   # Create PR on GitHub   ```### Quick Reference: Complete Workflow**Pull → Edit → Push to GitHub → Sync to Databricks**```bash# 1. Export from Databricks to localdatabricks workspace export '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --file practice-notebook-1.py --format SOURCE# 2. Make changes locally (edit the .py file)# 3. Commit to Gitgit add practice-notebook-1.pygit commit -m "Update notebook"git push origin main# 4. Import back to Databricks (if not using Repos)databricks workspace import practice-notebook-1.py '/Users/sagarrdass@outlook.com/databricks-practice/practice-notebook-1' --language PYTHON --format SOURCE --overwrite```**Note:** The `--overwrite` flag replaces the existing notebook in Databricks.### Quick Sync Scripts (Included in this Repo)This repository includes helper scripts to simplify the sync process:**1. Sync FROM Databricks TO local/GitHub:**```bash./sync-from-databricks.sh```This script will:- Export notebooks from Databricks to local files- Optionally commit changes to Git- Optionally push to GitHub**2. Sync FROM local/GitHub TO Databricks:**```bash./sync-to-databricks.sh```This script will:- Import local notebook files to Databricks workspace- Overwrite existing notebooks in Databricks**Typical Workflow:**```bash# Pull latest changes from Databricks./sync-from-databricks.sh# Make changes locally, commit and push to GitHubgit add .git commit -m "Update notebooks"git push origin main# After pulling changes from GitHub (e.g., from a teammate)git pull origin main./sync-to-databricks.sh```